{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8aff099d-a933-4df4-98a6-a9a583f2de73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "  html = urlopen('http://pythonscraping.com/pages/page1.htmll')\n",
    "except HTTPError as e:  \n",
    "  print(e)\n",
    "except URLError as e:  \n",
    "  print(\"The server could not be found!\")\n",
    "else:\n",
    "  print(\"It Worked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dab20bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\python312\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python312\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python312\\lib\\site-packages (from beautifulsoup4->bs4) (2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\14433\\appdata\\roaming\\python\\python312\\site-packages (from html5lib) (1.16.0)\n",
      "Collecting webencodings (from html5lib)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "   ---------------------------------------- 0.0/112.2 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/112.2 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 102.4/112.2 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 112.2/112.2 kB 1.3 MB/s eta 0:00:00\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, html5lib\n",
      "Successfully installed html5lib-1.1 webencodings-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55097faa-72a3-495c-8b1b-180f54cd5ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser') # (string that object is based on, specified parser to use)\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8dd1bd",
   "metadata": {},
   "source": [
    "**The scraper below comes with more advanced handling to handle potential errors such as:**\n",
    "\n",
    "  * HTTP error - url is found but file/filepath isn't found\n",
    "  * Url error - no url found\n",
    "  * Attribute error - missing attribute like h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2aaf008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    except URLError as e:\n",
    "        print(\"The server could not be found!\")\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        title = bs.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title == None:\n",
    "  print(\"Title could not be found\")\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e9128",
   "metadata": {},
   "source": [
    "**This section covers searching for tags by attributes, working with lists of tags, and navigating parse trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d24b179",
   "metadata": {},
   "source": [
    "This subsection will focus on parsing html that has css styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89387c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "# use find all to extract a python list of proper nouns found by selecting only the text within <specifiedTag></specifiedTag>\n",
    "name_list = bs.find_all('span', {'class':'green'}) # (tagName, tagAttributes)\n",
    "for name in name_list:\n",
    "  print(name.get_text()) # .get_text strips all tags from the document you are working with and returns a Unicode string containing the text (within those tags) only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94695f87",
   "metadata": {},
   "source": [
    "Calling '.get_text' should always be the last thing you do, immediately before you print, store, or manipulate your final data.\n",
    "In general, try to preserve the tag structure of a document as long as possible\n",
    "\n",
    "In this next subsection, 'find()' and 'find_all()' with 'BeautifulSoup' will be discussed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58a985",
   "metadata": {},
   "source": [
    "The functions are very similar:\n",
    "  * find_all(tag, attrs, recursive, text, limit, **kwargs)\n",
    "  * find(tag, attrs, recursive, text, **kwargs)\n",
    "\n",
    "95% of the time you will only need to use the first 2 arguments:\n",
    "  * tag\n",
    "  * attributes\n",
    "\n",
    "In find_all you can pass a sting tag name or a list of string tag names:\n",
    "  * find_all('h1')\n",
    "  * find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']) --> which returns a list of all the header tags in a document\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b491c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
